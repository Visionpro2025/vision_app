 # modules/orchestrator.py
from __future__ import annotations
from pathlib import Path
from datetime import datetime
import json
import traceback

import streamlit as st
import pandas as pd

from .logger_setup import get_logger

# ===== Rutas base y carpetas de trabajo
ROOT = Path(__file__).resolve().parent.parent
RUNS = ROOT / "__RUNS" / "ORCHESTRATOR"
RUNS.mkdir(parents=True, exist_ok=True)

LOG = get_logger("orchestrator")

# ====== utilidades cacheadas
@st.cache_data(show_spinner=False)
def _load_csv(path: Path) -> pd.DataFrame | None:
    try:
        return pd.read_csv(path, dtype=str, encoding="utf-8")
    except Exception as e:
        LOG.exception("Error leyendo %s", path)
        st.error(f"Error al leer {path.name}: {e}")
        return None

def _save_json(data: dict, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ====== chequeos de salud por capa
def _check_bibliography():
    corpus = ROOT / "__CORPUS" / "GEMATRIA"
    needed = ["bibliography.md"]
    missing = [f for f in needed if not (corpus / f).exists()]
    if missing:
        return False, f"Falta: {', '.join(missing)}"
    return True, "OK (bibliograf√≠a base presente)."
def _check_noticias():
    p = ROOT / "noticias.csv"
    if not p.exists():
        return False, "No se encontr√≥ noticias.csv"
    df = _load_csv(p)
    if df is None or df.empty:
        return False, "noticias.csv vac√≠o o ilegible"
    cols_req = {
        "id_noticia","fecha","sorteo","pais","fuente",
        "titular","resumen","etiquetas",
    }
    if not cols_req.issubset(set(df.columns)):
        faltan = cols_req - set(df.columns)
        return False, f"Columnas faltantes en noticias.csv: {', '.join(sorted(faltan))}"
    return True, f"OK ({len(df)} filas)."

def _check_t70():
    p = ROOT / "T70.csv"
    if not p.exists():
        return False, "No se encontr√≥ T70.csv"
    df = _load_csv(p)
    if df is None or df.empty:
        return False, "T70.csv vac√≠o o ilegible"
    return True, f"OK ({len(df)} filas)."

def _check_gematria():
    try:
        from . import gematria  # solo prueba de import
        return True, "OK (m√≥dulo importado)."
    except Exception as e:
        LOG.exception("Error importando gematria")
        return False, f"Fallo import gematria: {e}"

def _check_subliminal():
    try:
        from .subliminal_module import extraer_mensaje_subliminal  # noqa
        return True, "OK (m√≥dulo importado)."
    except Exception as e:
        LOG.exception("Error importando subliminal_module")
        return False, f"Fallo import subliminal: {e}"

# ====== pasos de pipeline
def step_noticias():
    ok, msg = _check_noticias()
    if not ok:
        raise RuntimeError(msg)
    LOG.info("Noticias listas: %s", msg)
    return msg

def step_gematria(dry_run: bool = True):
    """
    Placeholder: valida m√≥dulo. Si en el futuro agregas un batch,
    aqu√≠ leer√° noticias y generar√° __RUNS/GEMATRIA/*.csv
    """
    ok, msg = _check_gematria()
    if not ok:
        raise RuntimeError(msg)
    if dry_run:
        LOG.info("Gematr√≠a (dry-run): %s", msg)
        return "Gematr√≠a verificada (dry-run)."
    # Aqu√≠ ir√≠a la ejecuci√≥n real cuando exista funci√≥n batch
    return "Gematr√≠a ejecutada."

def step_subliminal():
    ok, msg = _check_subliminal()
    if not ok:
        raise RuntimeError(msg)

    from .subliminal_module import extraer_mensaje_subliminal  # usa tu backend real
    df = _load_csv(ROOT / "noticias.csv")
    assert df is not None
    out_rows = []
    for _, r in df.iterrows():
        text = " ".join([
            str(r.get("titular", "")),
            str(r.get("resumen", "")),
            str(r.get("etiquetas", "")),
        ])
        res = extraer_mensaje_subliminal(text)
        out_rows.append({
            "id_noticia": r.get("id_noticia", ""),
            "emocion": res["emocion"],
            "intensidad": res["intensidad"],
            "arquetipo": res["arquetipo"],
            "mensaje": res["mensaje"],
            "timestamp": datetime.utcnow().isoformat(timespec="seconds")+"Z",
        })
    df_out = pd.DataFrame(out_rows)
    out_path = RUNS / f"subliminal_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv"
    df_out.to_csv(out_path, index=False, encoding="utf-8")
    LOG.info("Subliminal exportado: %s", out_path.name)
    return f"Subliminal exportado: {out_path.name}"

def step_consolidar():
    """
    Placeholder: ejemplo para consolidar salidas de capas en un √∫nico reporte.
    """
    # Por ahora solo indica que el paso existe
    return "Consolidaci√≥n (placeholder)."

# ====== UI
def render_orchestrator():
    st.header("‚è±Ô∏è Orquestador de capas (pipeline)")
    st.caption(f"Base: `{ROOT}`  ¬∑  Runs: `__RUNS/ORCHESTRATOR`")

    # Estado r√°pido de salud
    with st.expander("‚úÖ Chequeos r√°pidos (salud de capas)", expanded=True):
        for name, fn in [
            ("üì∞ Noticias", _check_noticias),
            ("üìä T70", _check_t70),
            ("üî§ Gematr√≠a", _check_gematria),
            ("üåÄ Subliminal", _check_subliminal),
        ]:
            ok, msg = fn()
            icon = "‚úÖ" if ok else "‚ö†Ô∏è"
            st.write(f"**{name}** ‚Äî {icon} {msg}")

    st.write("---")
    st.subheader("‚ñ∂Ô∏è Ejecutar pipeline")

    colA, colB, colC, colD = st.columns(4)
    with colA:
        run_news = st.checkbox("Noticias", value=True)
    with colB:
        run_gem = st.checkbox("Gematr√≠a (dry-run)", value=True)
    with colC:
        run_sub = st.checkbox("Subliminal", value=True)
    with colD:
        run_cons = st.checkbox("Consolidar", value=False)

    run_btn = st.button("üöÄ Ejecutar ahora", use_container_width=True)

    if not run_btn:
        return

    # Bit√°cora de la corrida
    run_id = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    run_log = {
        "run_id": run_id,
        "started_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "steps": [],
        "status": "running",
    }
    log_path = RUNS / f"run_{run_id}.json"
    _save_json(run_log, log_path)

    steps = []
    if run_news: steps.append(("Noticias", step_noticias))
    if run_gem:  steps.append(("Gematr√≠a", step_gematria))
    if run_sub:  steps.append(("Subliminal", step_subliminal))
    if run_cons: steps.append(("Consolidaci√≥n", step_consolidar))

    prog = st.progress(0, text="Iniciando‚Ä¶")
    status_box = st.empty()

    try:
        for i, (label, fn) in enumerate(steps, start=1):
            status_box.info(f"Ejecutando **{label}** ‚Ä¶")
            LOG.info("Paso: %s", label)
            result_msg = fn()
            run_log["steps"].append({"step": label, "ok": True, "msg": result_msg})
            _save_json(run_log, log_path)
            prog.progress(int(i / max(len(steps),1) * 100), text=f"{label} ‚úì")

        run_log["status"] = "success"
        run_log["finished_at"] = datetime.utcnow().isoformat(timespec="seconds") + "Z"
        _save_json(run_log, log_path)
        st.success("Pipeline finalizado correctamente ‚úÖ")
        st.code(json.dumps(run_log, ensure_ascii=False, indent=2))

    except Exception as e:
        LOG.exception("Pipeline: error en ejecuci√≥n")
        run_log["status"] = "error"
        run_log["error"] = f"{type(e).__name__}: {e}"
        run_log["traceback"] = traceback.format_exc()
        run_log["finished_at"] = datetime.utcnow().isoformat(timespec="seconds") + "Z"
        _save_json(run_log, log_path)
        st.error(f"‚ùå Error: {e}")
        st.exception(e)
